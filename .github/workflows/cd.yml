name: CD

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Target environment"
        required: true
        default: "dev"
        type: choice
        options:
          - dev
          - prod

jobs:
  validate:
    name: Validate Before Deploy
    runs-on: ubuntu-latest
    env:
      PYTHONPATH: ${{ github.workspace }}/src:${{ github.workspace }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -r requirements/dev.txt
          python -m pip install -e .
          python -m pip install "apache-airflow==2.9.3" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.9.3/constraints-3.11.txt"

      - name: Run tests
        run: pytest -q

      - name: Parse DAGs
        run: python scripts/check_dags.py

  package:
    name: Package Deployable Artifacts
    runs-on: ubuntu-latest
    needs: [validate]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Create release bundle
        run: |
          mkdir -p dist
          tar -czf dist/lakehouse-bundle.tar.gz dags spark_jobs configs src

      - name: Upload release bundle
        uses: actions/upload-artifact@v4
        with:
          name: lakehouse-bundle-${{ inputs.environment }}
          path: dist/lakehouse-bundle.tar.gz

  deploy:
    name: Deploy Placeholder
    runs-on: ubuntu-latest
    needs: [package]
    environment: ${{ inputs.environment }}

    steps:
      - name: Deployment handoff
        run: |
          echo "Deploy hook placeholder for '${{ inputs.environment }}'"
          echo "Integrate this step with MWAA/Composer/Databricks or internal platform deployment tooling."
